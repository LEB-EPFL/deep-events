{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mitosplit_net import preprocessing, augmentation, evaluation, training, plotting, util\n",
    "from albumentations import Compose, Rotate, RandomRotate90, HorizontalFlip, Flip, ElasticTransform, GaussNoise, RandomCrop, Resize\n",
    "from tqdm import tqdm\n",
    "import os, sys\n",
    "from os import path\n",
    "import random as r\n",
    "from import_augmentation_function import import_fun, aug_fun, import_aug_fun\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS ALL IMAGES WITHIN THE FOLDER \"imager_dir\" ##\n",
    "\n",
    "files_dir = r'C:\\Users\\roumba\\Documents\\Software\\deep-events\\training_data'\n",
    "images_dir = '220915_mtstaygold_cos7_ZEISS_bf_pos'\n",
    "joined_path = os.path.join(files_dir, images_dir)\n",
    "img_size = 256\n",
    "data_ratio=0.1\n",
    "data_split_state = None \n",
    "number_of_augmentations = 5 \n",
    "\n",
    "#for some reason, even though it should have been deleted, the first two arrays of axis0 are zeros?# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519/519 [00:00<00:00, 1443.78it/s]\n",
      "100%|██████████| 519/519 [00:00<00:00, 1734.49it/s]\n",
      "100%|██████████| 519/519 [00:00<00:00, 1452.43it/s]\n",
      "100%|██████████| 519/519 [00:00<00:00, 1766.23it/s]\n",
      "100%|██████████| 519/519 [00:00<00:00, 1607.97it/s]\n"
     ]
    }
   ],
   "source": [
    "## THIS WORKS FOR A NETWORK WITH NO MEMORY ##\n",
    "\n",
    "all_image_array, all_image_array_gauss = import_fun(joined_path, files_dir, images_dir)\n",
    "augmentation_data, data_val, augmentation_data_gauss, data_gauss_val =  train_test_split(all_image_array, all_image_array_gauss, \n",
    "                                                                                                       test_size=data_ratio, random_state=data_split_state)\n",
    "data_aug = augmentation_data\n",
    "data_gauss_aug = augmentation_data_gauss\n",
    "for j in range(number_of_augmentations):\n",
    "    augment_data, augment_data_gauss = aug_fun(augmentation_data, augmentation_data_gauss)\n",
    "    data_aug = np.concatenate((data_aug, augment_data))\n",
    "    data_gauss_aug = np.concatenate((data_gauss_aug,augment_data_gauss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3756.54it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2502.97it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 2230.73it/s]\n",
      "100%|██████████| 29/29 [00:00<?, ?it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 648.68it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 2200.79it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1250.39it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 447.11it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 2935.73it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 923.04it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 2000.76it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 2000.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 2334.43it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 1753.16it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 808.07it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 3182.17it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 1000.64it/s]\n",
      "100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 2647.08it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 2643.33it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 2664.11it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 819.54it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 1256.55it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 2370.25it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 3527.33it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 2375.17it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 884.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## THIS WORKS FOR MEMORY CONSERVATION ##\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m all_data_aug, all_data_gauss_aug, all_image_array, all_image_array_gauss \u001b[39m=\u001b[39m import_aug_fun(joined_path, files_dir, images_dir)\n\u001b[0;32m      4\u001b[0m np\u001b[39m.\u001b[39mshape(all_data_aug)\n\u001b[0;32m      6\u001b[0m \u001b[39m## DATA SPLIT IN VALIDATION AND AUGMENTATION ##\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\roumba\\Documents\\Software\\deep-events\\import_augmentation_function.py:78\u001b[0m, in \u001b[0;36mimport_aug_fun\u001b[1;34m(joinpath, fdir, imdir)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39m#augmented data#\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     data_gauss_aug \u001b[39m=\u001b[39m augStack_one(image_array_gauss, transform, sigma\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     all_data_gauss_aug \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([all_data_gauss_aug, data_gauss_aug])\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(joined_image_path)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## THIS WORKS FOR MEMORY CONSERVATION ##\n",
    "\n",
    "all_data_aug, all_data_gauss_aug, all_image_array, all_image_array_gauss = import_aug_fun(joined_path, files_dir, images_dir)\n",
    "np.shape(all_data_aug)\n",
    "\n",
    "## DATA SPLIT IN VALIDATION AND AUGMENTATION ##\n",
    "\n",
    "val_size=int(data_ratio * np.size(all_data_aug,0))\n",
    "rand_arr= np.zeros(val_size)\n",
    "aug_size= np.size(all_data_aug,0) - val_size\n",
    "data_val= np.zeros((val_size, 256,256))\n",
    "data_gauss_val= np.zeros((val_size, 256,256))\n",
    "x = list(range(np.size(all_data_aug,0)-val_size))\n",
    "\n",
    "random_num_row = np.random.choice(x,val_size, replace=False)\n",
    "\n",
    "for i in range(val_size):\n",
    "    x= random_num_row[i]\n",
    "    data_val[i,:,:]= all_image_array[x,:,:]\n",
    "    data_gauss_val[i,:,:]= all_image_array_gauss[x,:,:]\n",
    "    if i==0:\n",
    "        data_aug = np.delete(all_data_aug, x, axis=0)\n",
    "        data_gauss_aug = np.delete(all_data_gauss_aug, x, axis=0)\n",
    "    else:\n",
    "        data_aug = np.delete(data_aug, x, axis=0)\n",
    "        data_gauss_aug = np.delete(data_gauss_aug, x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMALIZATION ##    \n",
    "\n",
    "data_aug_norm = data_aug \n",
    "data_gauss_aug_norm= data_gauss_aug\n",
    "data_val_norm = data_val \n",
    "data_gauss_val_norm= data_gauss_val\n",
    "\n",
    "for framenumber in range(np.size(data_val, 0)):\n",
    "    # k is the bit of data that we want to set to background and this should be reconsidered since maybe just quoting a number isn't very productive#\n",
    "    k=0.1 \n",
    "    kk=1/(1-k)\n",
    "\n",
    "    # validation data #\n",
    "    data_vall = (data_val[framenumber])/(np.max(data_val[framenumber]))                                               \n",
    "    data_vall = data_vall-k\n",
    "    data_vall[data_vall < 0] = 0   \n",
    "    data_val_norm[framenumber] = data_vall*kk\n",
    "\n",
    "    data_gauss_vall = (data_gauss_val[framenumber])/(np.max(data_gauss_val[framenumber])) \n",
    "    data_gauss_vall = data_gauss_vall-k\n",
    "    data_gauss_vall[data_gauss_vall < 0] = 0     \n",
    "    data_gauss_val_norm[framenumber] = data_gauss_vall*kk                                             \n",
    "    \n",
    "\n",
    "for framenumber in range(np.size(data_aug,0)):\n",
    "\n",
    "    # augmentation data #\n",
    "    data_augg = (data_aug[framenumber])/(np.max(data_aug[framenumber]))                                               \n",
    "    data_augg = data_augg-k\n",
    "    data_augg[data_augg < 0] = 0   \n",
    "    data_aug_norm[framenumber] = data_augg*kk\n",
    "\n",
    "    data_gauss_augg = (data_gauss_aug[framenumber])/(np.max(data_gauss_aug[framenumber]))\n",
    "    data_gauss_augg = data_gauss_augg-k\n",
    "    data_gauss_augg[data_gauss_augg < 0] = 0     \n",
    "    data_gauss_aug_norm[framenumber] = data_gauss_augg*kk     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHUFFLE ##\n",
    "\n",
    "r.shuffle(data_val_norm)\n",
    "r.shuffle(data_gauss_val_norm)\n",
    "r.shuffle(data_aug_norm)\n",
    "r.shuffle(data_gauss_aug_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "## DEFINE GPU DEVICE WHERE THE CODE WILL RUN ON ##\n",
    "\n",
    "gpu = tf.config.list_physical_devices('GPU')[0]\n",
    "print(gpu)\n",
    "tf.config.experimental.set_memory_growth(gpu, True)\n",
    "gpu = tf.device('GPU:0/') \n",
    "\n",
    "base_dir = r'C:\\Users\\roumba\\Documents\\Software\\deep-events'\n",
    "data_path = base_dir+ r'\\Norm_data' \n",
    "model_path = base_dir+ '\\Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ref_f8_c9_b16\n",
      "* Start Encoder Section *\n",
      "* Start Center Section *\n",
      "* Start Decoder Section *\n",
      "Epoch 1/20\n",
      "176/176 - 55s - loss: nan - binary_accuracy: 0.9224 - val_loss: nan - val_binary_accuracy: 0.9209 - 55s/epoch - 314ms/step\n",
      "Epoch 2/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 255ms/step\n",
      "Epoch 3/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 255ms/step\n",
      "Epoch 4/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 255ms/step\n",
      "Epoch 5/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 6/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 7/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 8/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 9/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 10/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 257ms/step\n",
      "Epoch 11/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 257ms/step\n",
      "Epoch 12/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 13/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 14/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 15/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 16/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 255ms/step\n",
      "Epoch 17/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 257ms/step\n",
      "Epoch 18/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 255ms/step\n",
      "Epoch 19/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 256ms/step\n",
      "Epoch 20/20\n",
      "176/176 - 45s - loss: nan - binary_accuracy: 0.9271 - val_loss: nan - val_binary_accuracy: 0.9209 - 45s/epoch - 257ms/step\n"
     ]
    }
   ],
   "source": [
    "with gpu:\n",
    "  nb_filters = 8\n",
    "  firstConvSize = 9\n",
    "  batch_size = [8, 16, 32, 256]\n",
    "  model, history= {}, {}\n",
    "  \n",
    "  b=batch_size[1]\n",
    "  model_name = 'ref_f%i_c%i_b%i'%(nb_filters, firstConvSize, b)\n",
    "  print('Model:', model_name)\n",
    "  model[model_name] = training.create_model(nb_filters, firstConvSize)\n",
    "  history[model_name] = training.train_model(model[model_name], data_aug_norm, data_gauss_aug_norm, b, data_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving C:\\Users\\roumba\\Documents\\Software\\deep-events\\Norm_data\\data_val.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [74], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## SAVE THE DATA ##\u001b[39;00m\n\u001b[0;32m      3\u001b[0m folder_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m util\u001b[39m.\u001b[39msave_h5(data_val_norm, data_path, \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata_val\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m util\u001b[39m.\u001b[39msave_h5(data_gauss_val_norm, data_path, \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata_gauss_val\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m util\u001b[39m.\u001b[39msave_h5(data_aug_norm, data_path, \u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata_aug\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\roumba\\documents\\software\\mitosplit-net\\mitosplit_net\\util.py:86\u001b[0m, in \u001b[0;36msave_h5\u001b[1;34m(data, path, name)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSaving \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfilename)\n\u001b[0;32m     85\u001b[0m hf \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39mFile(filename, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m hf\u001b[39m=\u001b[39m hf\u001b[39m.\u001b[39;49mcreate_dataset(name, data\u001b[39m=\u001b[39;49mdata, compression\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, compression_opts\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m)\n\u001b[0;32m     87\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\roumba\\Documents\\Software\\deep-events\\.env\\lib\\site-packages\\h5py\\_hl\\group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    158\u001b[0m         parent_path, name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mrsplit(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m         group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 161\u001b[0m dsid \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    162\u001b[0m dset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mDataset(dsid)\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\Users\\roumba\\Documents\\Software\\deep-events\\.env\\lib\\site-packages\\h5py\\_hl\\dataset.py:156\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     sid \u001b[39m=\u001b[39m h5s\u001b[39m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 156\u001b[0m dset_id \u001b[39m=\u001b[39m h5d\u001b[39m.\u001b[39;49mcreate(parent\u001b[39m.\u001b[39;49mid, name, tid, sid, dcpl\u001b[39m=\u001b[39;49mdcpl, dapl\u001b[39m=\u001b[39;49mdapl)\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m (data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    159\u001b[0m     dset_id\u001b[39m.\u001b[39mwrite(h5s\u001b[39m.\u001b[39mALL, h5s\u001b[39m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:87\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "## SAVE THE DATA ##\n",
    "\n",
    "folder_name = list(model.keys())\n",
    "\n",
    "util.save_h5(data_val_norm, data_path, '\\data_val')\n",
    "util.save_h5(data_gauss_val_norm, data_path, '\\data_gauss_val')\n",
    "util.save_h5(data_aug_norm, data_path, '\\data_aug')\n",
    "util.save_h5(data_gauss_aug_norm, data_path, '\\data_gauss_aug')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'name' and 'folder_name' lenghts don't match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\roumba\\documents\\software\\mitosplit-net\\mitosplit_net\\util.py:118\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, path, name, folder_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49msave(path\u001b[39m+\u001b[39mname\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'save'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m util\u001b[39m.\u001b[39msave_model(model, model_path, [\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(model), folder_name)\n\u001b[0;32m      2\u001b[0m util\u001b[39m.\u001b[39msave_pkl(history, model_path, [\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(model), folder_name)\n",
      "File \u001b[1;32mc:\\users\\roumba\\documents\\software\\mitosplit-net\\mitosplit_net\\util.py:121\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, path, name, folder_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m folder_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(folder_name)\u001b[39m!=\u001b[39m\u001b[39mlen\u001b[39m(name):\n\u001b[1;32m--> 121\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfolder_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m lenghts don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m     \u001b[39melif\u001b[39;00m folder_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(folder_name)\u001b[39m==\u001b[39m\u001b[39mlen\u001b[39m(name):\n\u001b[0;32m    123\u001b[0m         \u001b[39mfor\u001b[39;00m model_name, subfolder, title \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(model, folder_name, name):\n",
      "\u001b[1;31mValueError\u001b[0m: 'name' and 'folder_name' lenghts don't match."
     ]
    }
   ],
   "source": [
    "util.save_model(model, model_path, ['model']*len(model), folder_name)\n",
    "util.save_pkl(history, model_path, ['history']*len(model), folder_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1eaf8819518d82bc7e7f729f546a337f692f85bf6d00cdfaf0712e2fc6595813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
